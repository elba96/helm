{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76405af2-63db-431b-97b4-7bf1fb9d2ed4",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layer_of_interest = [1, 2, 3, 5, 10, 15, 18]\n",
    "\n",
    "COLORS = {\n",
    "    1: (255, 127, 14), # orange\n",
    "    3: (44, 160, 44), # green\n",
    "    18: (214, 39, 40), # red #D62728\n",
    "    2: (148, 103, 189), # purple #9467BD\n",
    "    5: (31, 119, 180), # blue #1F77B4\n",
    "    10: (188, 189, 34), # lime green #BCBD22\n",
    "    15: (23, 190, 207), # light blue #17BECF\n",
    "    4: (140, 86, 75), # brown #8C564B\n",
    "    6: (227, 119, 194), # pink #E377C2\n",
    "    7: (127, 127, 127), # grey #7F7F7F\n",
    "}\n",
    "\n",
    "COLORS_DARKER = {\n",
    "    1: (229, 118, 20), # orange\n",
    "    3: (31, 138, 44), # green\n",
    "    18: (176, 32, 32), # red #D62728\n",
    "    2: (148, 103, 189), # purple #9467BD\n",
    "    5: (31, 119, 180), # blue #1F77B4\n",
    "    10: (188, 189, 34), # lime green #BCBD22\n",
    "    15: (23, 190, 207), # light blue #17BECF\n",
    "    4: (140, 86, 75), # brown #8C564B\n",
    "    6: (227, 119, 194), # pink #E377C2\n",
    "    7: (127, 127, 127), # grey #7F7F7F\n",
    "}\n",
    "\n",
    "RAINBOW_COLORS = [\n",
    "    \"rgb(244, 67, 54)\",\n",
    "    \"rgb(232, 30, 99)\",\n",
    "    \"rgb(192, 35, 120)\",\n",
    "    \"rgb(169, 37, 150)\",\n",
    "    \"rgb(156, 39, 176)\",\n",
    "    \"rgb(103, 58, 183)\",\n",
    "    \"rgb(63, 81, 181)\",\n",
    "    \"rgb(33, 150, 243)\",\n",
    "    \"rgb(3, 169, 244)\",\n",
    "    \"rgb(0, 188, 212)\",\n",
    "    \"rgb(0, 150, 136)\",\n",
    "    \"rgb(76, 175, 80)\",\n",
    "    \"rgb(139, 195, 74)\",\n",
    "    \"rgb(205, 220, 57)\",\n",
    "    \"rgb(255, 235, 59)\",\n",
    "    \"rgb(255, 193, 7)\",\n",
    "    \"rgb(255, 152, 0)\",\n",
    "    \"rgb(255, 87, 34)\"\n",
    "]\n",
    "\n",
    "FONT_SIZE_TITLE = 24\n",
    "FONT_SIZE_AXIS = 22\n",
    "FONT_SIZE_TICKS = 20\n",
    "FONT_SIZE_LEGEND = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c421107-bc81-42f1-a09d-c1df6a512d07",
   "metadata": {},
   "source": [
    "# Analyze Causes\n",
    "Why are the results as they are? In this section we try to answer some question by analyzing the hidden states and attention patterns of the TrXL in different layers. To this end, a trained SHELM model was used to sample several example episodes of the Memory and Psychlab environments. During sampling the hidden states, attention distributions and rewards were saved and later visualized/ analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6d8e5-7aa1-4b1f-b96e-f8dc88e7db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "torch.cuda.manual_seed(101)\n",
    "random.seed(101)\n",
    "\n",
    "# if not os.path.exists(\"/home/dominik/Nextcloud/Dokumente/Studium/Master/Thesis/Results/investigation-data-mem.pkl\"):\n",
    "\n",
    "    \n",
    "    \n",
    "observations_path = os.path.join(\"data\", \"observations\")\n",
    "\n",
    "if os.path.exists(observations_path):\n",
    "    \n",
    "    observations_psych = sorted(\n",
    "    glob.glob(os.path.join(observations_path, \"Continuous-Recognition\", \"ep1\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "    observations_mem = sorted(\n",
    "        glob.glob(os.path.join(observations_path, \"Memory\", \"ep1\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "    observations_psych2 = sorted(\n",
    "        glob.glob(os.path.join(observations_path, \"Continuous-Recognition\", \"ep2\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "    observations_mem2 = sorted(\n",
    "        glob.glob(os.path.join(observations_path, \"Memory\", \"ep2\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "    observations_psych3 = sorted(\n",
    "        glob.glob(os.path.join(observations_path, \"Continuous-Recognition\", \"ep3\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "    observations_mem3 = sorted(\n",
    "        glob.glob(os.path.join(observations_path, \"Memory\", \"ep3\", \"*.png\")),\n",
    "        key=lambda x: int(x.split('step-')[-1].split('-')[0])\n",
    "    )\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Ensure the observations are downloaded with the script in `data/download_results.sh`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afe71a-5ecd-4071-bb22-9066a5b3b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b) / (norm(a) * norm(b))\n",
    "\n",
    "def load_model_output(env_v: str) -> Tuple[Dict[int, List[float]], Dict[int, List[float]], List[np.ndarray], Dict[int, List[float]], List[str]]:\n",
    "    \"\"\"\n",
    "    Load the model output data from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        env_v (str): The environment version.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, List[float]], Dict[int, List[float]], List[np.ndarray], Dict[int, List[float]], List[str]]:\n",
    "            A tuple containing the following data:\n",
    "            - cosine_distances (Dict[int, List[float]]): The cosine distances for all evaluated layers.\n",
    "            - l2_distances (Dict[int, List[float]]): The L2 distances for all evaluated layers.\n",
    "            - hidden_states (List[np.ndarray]): The hidden states for each timestep.\n",
    "            - attentions (Dict[int, List[float]]): The attention weights for all evaluated layers.\n",
    "            - tokens (List[str]): The tokens for the sampled episode.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the data file does not exist.\n",
    "\n",
    "    Note:\n",
    "        Ensure the data is downloaded with the script in `data/download_results.sh`.\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(\"data\", \"investigation-data\", f\"investigation-data-{env_v}.pkl\")\n",
    "    if os.path.exists(data_path):\n",
    "        with open(data_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            cosine_distances = data[\"cosine_distances\"]\n",
    "            l2_distances = data[\"l2_distances\"]\n",
    "            hidden_states = data[\"hidden_states\"]\n",
    "            attentions = data[\"attention\"]\n",
    "            tokens = data[\"tokens\"]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Ensure the data is downloaded with the script in `data/download_results.sh`\")\n",
    "\n",
    "    return cosine_distances, l2_distances, hidden_states, attentions, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e412e-b16b-447b-8b31-c42401e179c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_mem, l2_distances_mem, hidden_states_mem, attention_mem, tokens_mem = \\\n",
    "load_model_output(\"mem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4652497-5304-4b1f-9cd7-65ad881af969",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_mem2, l2_distances_mem2, hidden_states_mem2, attention_mem2, tokens_mem2 = \\\n",
    "load_model_output(\"mem2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79add21a-728a-417d-9953-6cf0c7ef421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_mem3, l2_distances_mem3, hidden_states_mem3, attention_mem3, tokens_mem3 = \\\n",
    "load_model_output(\"mem3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f084e09-9288-49d4-aeb5-1a69ff473efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_psy, l2_distances_psy, hidden_states_psy, attention_psy, tokens_psy = \\\n",
    "load_model_output(\"psy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee511616-9058-40ee-9f0c-53aca9822782",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_psy2, l2_distances_psy2, hidden_states_psy2, attention_psy2, tokens_psy2 = \\\n",
    "load_model_output(\"psy2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b88ffe-7448-4c58-91a0-0596f846bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances_psy3, l2_distances_psy3, hidden_states_psy3, attention_psy3, tokens_psy3 = \\\n",
    "load_model_output(\"psy3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dcda7-fd57-4fa9-9b10-d2314e4c489e",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Visualizing the attention mechanism in a transformer can be quite challenging considering there are multiple layers each with multiple attention heads. In the case of the TrXL, the default model has 18 hidden layers, each containing 16 attention heads.\n",
    "### Softmax Distribution\n",
    "One way of showing differences in the attention heads is by comparing the softmax distribution of heads. This can be done with a violin plot to actually show the distribution or by summing up the softmax and counting how many tokens are necessary to reach a certain threshold. This way attention heads can loosely be categorized by how big their window of attention is and if they only focus only on single tokens are attend to a more tokens. Furthermore we also tried to show to which token in the memory the input token attends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbc50b-48e4-4e62-a8ca-ddef049602fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_softmax_comparison(\n",
    "        attention: Dict[int, List[float]],\n",
    "        negative: int,\n",
    "        positive: int, \n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plot a comparison of the softmax values for two layers in the attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        attention (Dict[int, List[float]]): The attention data, where the layers and the values are the attention weights over all timesteps.\n",
    "        negative (int): The layer used for the negative side of the plot.\n",
    "        positive (int): The layer used for the positive side of the plot.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: The plot showing the softmax values for each step and head for the negative and positive layers.\n",
    "\n",
    "    Note:\n",
    "        The plot is a grid of violin plots, with one violin plot per step and head.\n",
    "        The violin plots show the distribution of softmax values for the negative and positive layers.\n",
    "        The violin plots are colored according to the layer (negative: green, positive: red).\n",
    "    \"\"\"\n",
    "    row_titles = [f\"Head {i}\" for i in range(1,17)]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=16, cols=12, shared_yaxes=True, shared_xaxes=True,\n",
    "        row_titles=row_titles,\n",
    "        vertical_spacing=0.0, horizontal_spacing=0.0,\n",
    "    )\n",
    "    \n",
    "    show_legend = True\n",
    "\n",
    "    for row in range(16):\n",
    "        for col in range(12):\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Step {col+1}\"]*len(attention[negative][col][row]),\n",
    "                    y=attention[negative][col][row],\n",
    "                    legendgroup=f'Layer {negative}', scalegroup=f'Layer {negative}', name=f'Layer {negative}',\n",
    "                    side='negative',\n",
    "                    line_color=\"rgb(44, 160, 44)\",\n",
    "                    showlegend=show_legend and row == 0,\n",
    "                    width=0.5,\n",
    "                    points=False,\n",
    "                ),\n",
    "                row=row+1,\n",
    "                col=col+1\n",
    "            )\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Step {col+1}\"]*len(attention[positive][col][row]),\n",
    "                    y=attention_mem[positive][col][row],\n",
    "                    legendgroup=f'Layer {positive}', scalegroup=f'Layer {positive}', name=f'Layer {positive}',\n",
    "                    side='positive',\n",
    "                    line_color=\"rgb(214, 39, 40)\",\n",
    "                    showlegend=show_legend and row == 0,\n",
    "                    width=0.5,\n",
    "                    points=False,\n",
    "                ),\n",
    "                row=row+1,\n",
    "                col=col+1\n",
    "            )\n",
    "            show_legend = False\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Attention Softmax Distribution - Layer {negative} vs. Layer {positive} - Memory\",\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.996,\n",
    "        ),\n",
    "\n",
    "        xaxis=dict(\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            visible=False,\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.032,\n",
    "            xanchor=\"center\",\n",
    "            x=.5,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            font_size=FONT_SIZE_LEGEND\n",
    "        ),\n",
    "        violingap=0,\n",
    "        violinmode='overlay',\n",
    "        height=2000,\n",
    "        width=1100,\n",
    "        margin=dict(\n",
    "            l=40,\n",
    "            r=20,\n",
    "            b=30,\n",
    "            t=40,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    fig.for_each_annotation(\n",
    "        lambda a: a.update(x = -0.035, textangle=-90) if a.text in row_titles\n",
    "        else()\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc10409-26c9-4cb4-b156-98011dfef079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps_slider(\n",
    "        attention: Dict[int, List[float]],\n",
    "        title: str,\n",
    "        threshold: float=0.9\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plot heatmaps with a slider to toggle between different layers of attention. The heatmap shows the number of tokens necessary to sumup the softmax distribution to at least the threshold.\n",
    "\n",
    "    Args:\n",
    "        attention (Dict[int, List[float]]): A dictionary containing attention weights for different layers.\n",
    "        title (str): The title of the plot.\n",
    "        threshold (float, optional): The threshold for filtering attention weights. Defaults to 0.9.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: The plot object.\n",
    "\n",
    "    Note:\n",
    "        - The function creates a heatmap for each layer and adds a slider to toggle between the layers.\n",
    "    \"\"\"\n",
    "\n",
    "    num_tokens = {i: [] for i in layer_of_interest}\n",
    "    for l in layer_of_interest:\n",
    "        tmp_tokens = []\n",
    "        try:\n",
    "            for heads in attention[l]:\n",
    "                sorted_attention = -np.sort(-heads, axis=-1)\n",
    "                cumsum = np.cumsum(sorted_attention, axis=1)\n",
    "                tmp_tokens.append(np.where(cumsum >= threshold, False, True).sum(axis=1) + 1)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "        num_tokens[l] = np.stack(tmp_tokens, axis=1)\n",
    "\n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    keys = list(attention.keys())\n",
    "    num_steps = len(attention[keys[0]])\n",
    "    \n",
    "    for l in attention:\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=num_tokens[l],\n",
    "                x=[f\"Step {i+1}\" for i in range(num_steps)],\n",
    "                y=[f\"Head {i+1}\" for i in range(16)],\n",
    "                colorscale='Inferno',\n",
    "                zmin=1,\n",
    "                zmax=16 if \"Memory\" in title else 255,\n",
    "                visible=False,\n",
    "                colorbar_tickfont_size=FONT_SIZE_LEGEND\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Make 0th trace visible\n",
    "    fig.data[0].visible = True\n",
    "    \n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i, l in enumerate(attention):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            label=f\"Layer {l}\",\n",
    "            args=[{\"visible\": [False] * len(fig.data)}]\n",
    "        )\n",
    "\n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "\n",
    "        steps.append(step)\n",
    "    \n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    if \"Memory\" in title:\n",
    "        fig.for_each_trace(lambda a: a.update(text=a.z, texttemplate=\"%{text}\", hovertemplate=None))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.985,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            traceorder=\"normal\",\n",
    "            font_size=FONT_SIZE_LEGEND\n",
    "        ),\n",
    "        height=700,\n",
    "        width=800,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=40,\n",
    "            b=20,\n",
    "            t=40,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d734a0-e04e-4c7e-8533-ab21e6f08465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmaps_over_tokens_slider(\n",
    "        attention: Dict[int, List[float]],\n",
    "        layer: int,\n",
    "        tokens: List[str],\n",
    "        title: str\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a plot of attention heatmaps over tokens with a slider to navigate through time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - attention (Dict[int, List[float]]): A dictionary containing attention weights for each layer and time step.\n",
    "    - layer (int): The layer of attention to plot.\n",
    "    - tokens (List[str]): The list of tokens.\n",
    "    - title (str): The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    - fig (go.Figure): The plot of attention heatmaps over tokens with a slider.\n",
    "\n",
    "    Note:\n",
    "    - The plot includes a slider to navigate through the time steps.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    mem_len = 16 if \"Memory\" in title else 256\n",
    "    for idx, timestep in enumerate(attention[layer]):\n",
    "        mem_tokens = [\"<EMPTY>\"] * (mem_len-(idx+1)) + tokens[:idx+1]\n",
    "        \n",
    "        n_heads, n_steps = timestep.shape\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=np.flip(timestep, 1),\n",
    "                x=[f\"Token {i+1}\" for i in range(n_steps)],\n",
    "                y=[f\"Head {i+1}\" for i in range(n_heads)][::-1],\n",
    "                colorscale='turbo',\n",
    "                zmin=0.,\n",
    "                zmax=1.,\n",
    "                visible=False,\n",
    "                colorbar_tickfont_size=FONT_SIZE_LEGEND\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Make 0th trace visible\n",
    "    fig.data[0].visible = True\n",
    "    \n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i, l in enumerate(attention[layer]):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            label=f\"Timestep {i+1}\",\n",
    "            args=[{\"visible\": [False] * len(fig.data)}]\n",
    "        )\n",
    "\n",
    "        step[\"args\"][0][\"visible\"][i] = True \n",
    "\n",
    "        steps.append(step)\n",
    "    \n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        pad={\"t\": 50 if \"Memory\" in title else 80},\n",
    "        steps=steps\n",
    "    )]\n",
    "    \n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.985,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            traceorder=\"normal\",\n",
    "            font_size=FONT_SIZE_LEGEND\n",
    "        ),\n",
    "        height=700,\n",
    "        width=1000,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=40,\n",
    "            b=20,\n",
    "            t=40,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b522b-4cf3-4229-ae27-15c253add84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_attention_per_layer_slider(\n",
    "        attention: Dict[int, List[float]],\n",
    "        tokens: List[str],\n",
    "        layer: int,\n",
    "        env: str\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a plot of the maximum attention per head for each time step in a given layer using a slider.\n",
    "    The left side represents the tokens retrieved from the input and the right side represents the tokens already in the memory of the TrXL.\n",
    "\n",
    "    Parameters:\n",
    "    - attention (Dict[int, List[float]]): A dictionary containing attention weights for each layer and time step.\n",
    "    - tokens (List[str]): The list of tokens.\n",
    "    - layer (int): The layer of attention to plot.\n",
    "    - env (str): The environment name.\n",
    "\n",
    "    Returns:\n",
    "    - fig (go.Figure): The plot of the maximum attention per head for each time step in the given layer with a slider.\n",
    "\n",
    "    Note:\n",
    "    - The plot includes a slider to navigate through the time steps.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for idx, timestep in enumerate(attention[layer]):\n",
    "        \n",
    "        n_heads, n_steps = timestep.shape\n",
    "        mem_len = 16 if \"Memory\" in env else 256\n",
    "        \n",
    "        if idx < mem_len:\n",
    "            mem_tokens = [\"<EMPTY>\"] * (mem_len-(idx+1)) + tokens[:idx+1]\n",
    "            \n",
    "        else:\n",
    "            start_idx = idx - mem_len +1\n",
    "            end_idx = start_idx + mem_len\n",
    "            mem_tokens = tokens[start_idx:end_idx]\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Parcoords(\n",
    "                visible=False,\n",
    "                line = dict(\n",
    "                    color=np.max(timestep, axis=1),\n",
    "                    colorscale=\"turbo\",\n",
    "                    showscale=True,\n",
    "                    cmin=0.,\n",
    "                    cmax=1.\n",
    "                ),\n",
    "                dimensions = list([\n",
    "                    dict(\n",
    "                        range = [0,len(tokens)],\n",
    "                        #constraintrange = [1,1],\n",
    "                        tickvals=[i for i in range(1,len(tokens)+1)],\n",
    "                        ticktext=[f\"{i+1}. {token}\" for i, token in enumerate(tokens)],\n",
    "                        label='Input Token',\n",
    "                        values=[idx+1 for _ in range(n_heads)]\n",
    "                    ),\n",
    "                    dict(\n",
    "                        range = [0,mem_len],\n",
    "                        #constraintrange = [1,1],\n",
    "                        tickvals=[i for i in range(1,mem_len+1)],\n",
    "                        ticktext=[f\"{i}. {token}\" for i, token in enumerate(mem_tokens, 1)],#[::-1],\n",
    "                        label='Memory',\n",
    "                        values=np.argmax(timestep, axis=1)+1\n",
    "                    )\n",
    "                ]),\n",
    "                #unselected = dict(line = dict(color = 'green', opacity = 0.0))\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.data[0].visible = True\n",
    "    \n",
    "    \n",
    "    steps = []\n",
    "    for i in range(idx+1):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            label=f\"Timestep {i+1}\",\n",
    "            args=[{\"visible\": [False] * len(fig.data)}]\n",
    "        )\n",
    "    \n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    \n",
    "        steps.append(step)\n",
    "    \n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "    \n",
    "    \n",
    "    fig.update_layout(\n",
    "        plot_bgcolor = 'white',\n",
    "        paper_bgcolor = 'white'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        title=f\"Max. Attention Per Head for Layer {layer} - {env}\",\n",
    "        title_x=0.5,\n",
    "        title_y=0.985,\n",
    "        title_xanchor=\"center\",\n",
    "        title_yanchor=\"top\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.15,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            traceorder=\"normal\"\n",
    "        ),\n",
    "        height=700 if \"Memory\" in env else 1000,\n",
    "        width=1000,\n",
    "        margin=dict(\n",
    "            l=100,\n",
    "            r=40,\n",
    "            b=20,\n",
    "            t=40,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384057f-8792-4f97-86ae-f4ae684b5148",
   "metadata": {},
   "source": [
    "#### Memory Environment\n",
    "##### Episode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489495c-3a46-426f-ad1a-5cd3dbd8bcaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_softmax_comparison(attention_mem, 3, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199fcef-5cbb-411b-a84d-c6112b2c33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_mem, \"No. Tokens for Softmax >= 0.9 - Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b790fb-60e3-4313-af8f-2a93565a6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_mem, 3, tokens_mem,\"Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900416c-dae8-4333-a0d7-d699b6e92b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_mem, tokens_mem, 3, \"Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c60174-ba3f-45c7-a7b0-7e3d5b3e2a22",
   "metadata": {},
   "source": [
    "##### Episode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad7ee1-afcc-49f7-bc9e-8cb849f30434",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_softmax_comparison(attention_mem2, 3, 18).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3770f-0c42-438e-9766-9807746d050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_mem2, \"No. Tokens for Softmax >= 0.9 - Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae26570-e701-4ad0-b246-f2ce90fbd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_mem2, 3, tokens_mem2, \"Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2991dfc-d66c-45b4-ad6d-2424a459cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_mem2, tokens_mem2, 3, \"Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbfc6df-1a27-4e7c-b9c0-faa784e0c7df",
   "metadata": {},
   "source": [
    "##### Episode 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7f815-5b02-41f8-9114-6e228a9acef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_softmax_comparison(attention_mem3, 3, 18).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589aea7-759e-4743-979c-b2a356f362fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_mem3, \"No. Tokens for Softmax >= 0.9 - Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf033b0-96db-4566-a376-0f7bcf4e46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_mem3, 3, tokens_mem3, \"Memory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2879eeb-494d-42cf-8c98-6c7aad086372",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_mem3, tokens_mem3, 15, \"Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e00341-aa48-4812-a2ef-6ea221972e6a",
   "metadata": {},
   "source": [
    "#### Continuous Recognition Environment\n",
    "Not all above visualization are suitable for visualizing the softmax distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4ae2d-bcdc-4399-baac-28419bd83a32",
   "metadata": {},
   "source": [
    "##### Episode 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d37f4b-868c-4058-9a50-018aa24b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_psy, \"No. Tokens for Softmax >= 0.9 - Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7099a-a39b-4bbe-b873-95eb4d944f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_psy, 3, tokens_psy, \"Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012a6f0-b3f6-4b1b-a318-c18beba6b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_psy, tokens_psy, 5, \"Continuous Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf10ef5-8cab-4450-96b2-22a8faee8839",
   "metadata": {},
   "source": [
    "##### Episode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65a096-9516-4dfe-9127-864c8e8ca1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_psy2, \"No. Tokens for Softmax >= 0.9 - Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218593db-b292-4ba9-8be2-a625146c5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_psy2, 18, tokens_psy2, \"Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9575b39-918b-4351-a322-baf4e206cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_psy2, tokens_psy2, 3, \"Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92220bba-9434-4e91-aa2f-4d5905551271",
   "metadata": {},
   "source": [
    "##### Episode 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002df0db-a83f-4a0e-a7dd-fdd8b5390b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_slider(attention_psy3, \"No. Tokens for Softmax >= 0.9 - Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25f8e7-1a1e-429b-8a1d-655fc77a12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_heatmaps_over_tokens_slider(attention_psy3, 18, tokens_psy3, \"Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb7ea9-3405-4dc4-8024-39a8933ae004",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_attention_per_layer_slider(attention_psy3, tokens_psy3, 10, \"Continuous Recognition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b57fa1-fb47-4b5e-9aa3-aba577bf54a5",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "Seeing the different attention pattern in the different heads raises the question which heads are the most important ones. To analyze this a trained SHELM model was was initialized with one of the 16 heads masked out. This model was then used to sample 50 episodes while collecting the rewards. Due to limited compute resources not all possible head maskings could be evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80a826-97f2-4167-b9c2-5baefd947894",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_path = os.path.join(\"data\", \"rewards\")\n",
    "\n",
    "if os.path.exists(reward_path):\n",
    "    rews_mem = np.load(os.path.join(reward_path, \"Memory-Rewards.npy\"))\n",
    "    rews_psy = np.load(os.path.join(reward_path, \"Psychlab-Rewards.npy\"))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Ensure the reward data is downloaded with the script in `data/download_results.sh`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a644f-eb9e-4faf-a953-28f8c23ef185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_head_mask(\n",
    "        data: np.ndarray,\n",
    "        title: str\n",
    "    ) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a box plot of accumulated rewards if sayed head is masked out during inference.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array containing the accumulated rewards for each head.\n",
    "    - title (str): The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "    - fig (go.Figure): The box plot of accumulated rewards for each head with the option to mask out a specific head.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i, rews in enumerate(data, 1):\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=f\"Head {i}\",\n",
    "                y=rews,\n",
    "                marker_color=RAINBOW_COLORS[i-1],\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Masked Out Head\",\n",
    "        yaxis_title=\"Accumulated Reward\",\n",
    "        title_x=0.5,\n",
    "        title_y=0.985,\n",
    "        title_xanchor=\"center\",\n",
    "        title_yanchor=\"top\",\n",
    "        legend_traceorder=\"reversed\",\n",
    "        height=700,\n",
    "        width=900,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=20,\n",
    "            b=20,\n",
    "            t=40,\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        range=[0., 1.] if \"Memory\" in title else [0., 55.]\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e2d36-5bf0-439f-b8d2-04924459cae7",
   "metadata": {},
   "source": [
    "#### Memory Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659dd41-49ec-4e28-b4aa-4c4ad37751db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_head_mask(rews_mem, \"Reward with Attention Head masked out - Memory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8c038-6121-45bd-a80a-989ed71e7ee3",
   "metadata": {},
   "source": [
    "#### Continuous Recognition Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28490132-cae7-424a-82d3-32e2fde5173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_head_mask(rews_psy, \"Reward with Attention Head masked out - Continuous Recognition\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
