{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76405af2-63db-431b-97b4-7bf1fb9d2ed4",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import List, Dict, Union, Tuple, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from rliable import library as rly\n",
    "from rliable import metrics\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layer_of_interest = [1, 2, 3, 5, 10, 15, 18]\n",
    "\n",
    "COLORS = {\n",
    "    1: (255, 127, 14), # orange\n",
    "    3: (44, 160, 44), # green\n",
    "    18: (214, 39, 40), # red #D62728\n",
    "    2: (148, 103, 189), # purple #9467BD\n",
    "    5: (31, 119, 180), # blue #1F77B4\n",
    "    10: (188, 189, 34), # lime green #BCBD22\n",
    "    15: (23, 190, 207), # light blue #17BECF\n",
    "    4: (140, 86, 75), # brown #8C564B\n",
    "    6: (227, 119, 194), # pink #E377C2\n",
    "    7: (127, 127, 127), # grey #7F7F7F\n",
    "}\n",
    "\n",
    "COLORS_DARKER = {\n",
    "    1: (229, 118, 20), # orange\n",
    "    3: (31, 138, 44), # green\n",
    "    18: (176, 32, 32), # red #D62728\n",
    "    2: (148, 103, 189), # purple #9467BD\n",
    "    5: (31, 119, 180), # blue #1F77B4\n",
    "    10: (188, 189, 34), # lime green #BCBD22\n",
    "    15: (23, 190, 207), # light blue #17BECF\n",
    "    4: (140, 86, 75), # brown #8C564B\n",
    "    6: (227, 119, 194), # pink #E377C2\n",
    "    7: (127, 127, 127), # grey #7F7F7F\n",
    "}\n",
    "\n",
    "RAINBOW_COLORS = [\n",
    "    \"rgb(244, 67, 54)\",\n",
    "    \"rgb(232, 30, 99)\",\n",
    "    \"rgb(192, 35, 120)\",\n",
    "    \"rgb(169, 37, 150)\",\n",
    "    \"rgb(156, 39, 176)\",\n",
    "    \"rgb(103, 58, 183)\",\n",
    "    \"rgb(63, 81, 181)\",\n",
    "    \"rgb(33, 150, 243)\",\n",
    "    \"rgb(3, 169, 244)\",\n",
    "    \"rgb(0, 188, 212)\",\n",
    "    \"rgb(0, 150, 136)\",\n",
    "    \"rgb(76, 175, 80)\",\n",
    "    \"rgb(139, 195, 74)\",\n",
    "    \"rgb(205, 220, 57)\",\n",
    "    \"rgb(255, 235, 59)\",\n",
    "    \"rgb(255, 193, 7)\",\n",
    "    \"rgb(255, 152, 0)\",\n",
    "    \"rgb(255, 87, 34)\"\n",
    "]\n",
    "\n",
    "FONT_SIZE_TITLE = 24\n",
    "FONT_SIZE_AXIS = 22\n",
    "FONT_SIZE_TICKS = 20\n",
    "FONT_SIZE_LEGEND = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc97cb-1d72-4eff-a456-2a766bb0a1c2",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "The training data of this work can be viewed directly in [wandb](https://wandb.ai/elba/Master-Thesis/table?nw=zglvzydk8v9). To run this notebook please first download the data with the script in `data/download_results.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a84f8-abe1-4621-b714-3e3c10eba0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run_data = os.path.join(\"data\", \"training-runs\", \"run_df.csv\")\n",
    "if os.path.exists(train_run_data):\n",
    "    run_df = pd.read_csv(train_run_data)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Ensure the training data is downloaded with the script in `data/download_results.sh`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d8655-057d-44c1-a4b4-e41321b6b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e8fac-16be-467e-8e79-8fa6e424c176",
   "metadata": {},
   "source": [
    "# Transform Data\n",
    "The data for the `rliable` library needs to be in a specific shape to calculate the IQM and CI. Also other investigations require a specific format or some form of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b578d69-bb07-498f-a5df-0f0185bf205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_names = {\n",
    "    'MiniGrid-MemoryS11-v0': 'Memory',\n",
    "    'psychlab_continuous_recognition': \"Continuous Recognition\"\n",
    "}\n",
    "\n",
    "tech_names = {v: k for k, v in canonical_names.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a57b7-e0c8-41f0-846e-6a1b5b98b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general data for showing training curves with IQM\n",
    "run_dict = {}\n",
    "for env in canonical_names:\n",
    "    name = canonical_names[env]\n",
    "    run_dict[name] = {}\n",
    "    tmp_df = run_df.query(\"`env`==@env\")\n",
    "    for n in tmp_df[\"setting\"].unique():\n",
    "        tmp_data = (run_df\n",
    "                   .query(\"`setting`== @n & `env` == @env\")\n",
    "                   .pivot(columns=\"name\", values=[\"rollout/ep_rew_mean\"], index=\"global_step\"))\n",
    "    \n",
    "        run_dict[name][n] = tmp_data.values.T\n",
    "\n",
    "iqm = lambda scores: np.array([metrics.aggregate_iqm(scores[:,i]) for i in range(scores.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320b094-91d4-40b5-8309-4cb98761ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for inspecting runtime vs. performance\n",
    "runtime_test = {}\n",
    "for env in run_df[\"env\"].unique():\n",
    "    runtime_test[env] = {}\n",
    "    for n1 in sorted(run_dict[canonical_names[env]]):\n",
    "        end_rew = np.expand_dims(np.array(run_dict[canonical_names[env]][n1][:,-1]), axis=1)\n",
    "        runtime_test[env][n1] = end_rew\n",
    "    \n",
    "    iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "        runtime_test[env],\n",
    "        iqm,\n",
    "        reps=50\n",
    "    )\n",
    "\n",
    "    runtime_test[env][\"iqm_scores\"] = iqm_scores\n",
    "    runtime_test[env][\"iqm_cis\"] = iqm_cis\n",
    "\n",
    "\n",
    "perf_vs_time = {}\n",
    "for env in run_df[\"env\"].unique():\n",
    "    perf_vs_time[env] = {}\n",
    "    for se in runtime_test[env][\"iqm_scores\"]:\n",
    "        perf_vs_time[env][se] = {}\n",
    "        perf_vs_time[env][se][\"iqm\"] = runtime_test[env][\"iqm_scores\"][se].item()\n",
    "        perf_vs_time[env][se][\"ci_low\"] = runtime_test[env][\"iqm_cis\"][se][0].item()\n",
    "        perf_vs_time[env][se][\"ci_high\"] = runtime_test[env][\"iqm_cis\"][se][1].item()\n",
    "        perf_vs_time[env][se][\"runtime\"] = run_df.query(\"`setting` == @se & `host` == 'celantur-delta' & `env` == @env\")[\"runtime\"].mean()\n",
    "        perf_vs_time[env][se][\"std\"] = run_df.query(\"`setting` == @se & `host` == 'celantur-delta' & `env` == @env\")[\"runtime\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932b0a9-f5b4-4888-a1a9-12e7b5e1441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for comparing update and rollout times\n",
    "rollout_data = os.path.join(\"data\", \"rollout-times\", \"rollout_times.pkl\")\n",
    "update_data = os.path.join(\"data\", \"update-times\", \"update_times.pkl\")\n",
    "\n",
    "if os.path.exists(rollout_data):\n",
    "    with open(rollout_data, \"rb\") as f:\n",
    "        rollout_times = pickle.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Ensure the data is downloaded with the script in `data/download_results.sh`\")\n",
    "\n",
    "if os.path.exists(update_data):\n",
    "    with open(update_data, \"rb\") as f:\n",
    "        update_times = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896716b6-dcf6-498c-b404-b0fff8991639",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqm = lambda scores: np.array([metrics.aggregate_iqm(scores[:,i]) for i in range(scores.shape[-1])])\n",
    "\n",
    "runtime_test = {}\n",
    "for env in run_df[\"env\"].unique():\n",
    "    runtime_test[env] = {}\n",
    "    for n1 in sorted(run_dict[canonical_names[env]]):\n",
    "        end_rew = np.expand_dims(np.array(run_dict[canonical_names[env]][n1][:,-1]), axis=1)\n",
    "        runtime_test[env][n1] = end_rew\n",
    "    \n",
    "    iqm_scores, iqm_cis = rly.get_interval_estimates(\n",
    "        runtime_test[env],\n",
    "        iqm,\n",
    "        reps=50\n",
    "    )\n",
    "\n",
    "    runtime_test[env][\"iqm_scores\"] = iqm_scores\n",
    "    runtime_test[env][\"iqm_cis\"] = iqm_cis\n",
    "\n",
    "perf_vs_time = {}\n",
    "for env in run_df[\"env\"].unique():\n",
    "    perf_vs_time[env] = {}\n",
    "    for se in runtime_test[env][\"iqm_scores\"]:\n",
    "        perf_vs_time[env][se] = {}\n",
    "        perf_vs_time[env][se][\"iqm\"] = runtime_test[env][\"iqm_scores\"][se].item()\n",
    "        perf_vs_time[env][se][\"ci_low\"] = runtime_test[env][\"iqm_cis\"][se][0].item()\n",
    "        perf_vs_time[env][se][\"ci_high\"] = runtime_test[env][\"iqm_cis\"][se][1].item()\n",
    "        perf_vs_time[env][se][\"runtime\"] = run_df.query(\"`setting` == @se & `host` == 'celantur-delta' & `env` == @env\")[\"runtime\"].mean()\n",
    "        perf_vs_time[env][se][\"std\"] = run_df.query(\"`setting` == @se & `host` == 'celantur-delta' & `env` == @env\")[\"runtime\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d651af1-df66-4465-8e42-83c3e300f5a4",
   "metadata": {},
   "source": [
    "# Evaluate Results\n",
    "How to the different SHELM variant compare? In this section we visualize the results after training each model variant on 10 different random seeds. The results of our experiments can be summarized by the IQM+CI of the collected reward during training as well the a comparison between the performance and runtime of each model variant.\n",
    "\n",
    "## IQM + CI\n",
    "The IQM + CI is calculated with the `rliable` package and should give a robut estimate on how good a RL model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d5b1f-5e07-45c1-9538-50ce04fcc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iqm(\n",
    "        x: List[float],\n",
    "        y: Dict[str, Union[List[float], np.ndarray]],\n",
    "        y_error: Dict[str, Tuple[List[float], List[float]]],\n",
    "        title: str,\n",
    "        key: Optional[Callable[[str], Tuple[int, float]]] = None\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plots the IQM (Inter Quartile Mean) + CI (Confidence Interval) over time.\n",
    "\n",
    "    Args:\n",
    "        x (List[float]): The x-axis values.\n",
    "        y (Dict[str, Union[List[float], np.ndarray]]): A dictionary containing the y-axis values for each experiment.\n",
    "            The dictionary should have the following structure:\n",
    "            {\n",
    "                name: y_values\n",
    "            }\n",
    "            where `name` is a string representing the name of the experiment and `y_values` is a list or numpy array of y-axis values.\n",
    "        y_error (Dict[str, Tuple[List[float], List[float]]]): A dictionary containing the error bars for each experiment.\n",
    "            The dictionary should have the following structure:\n",
    "            {\n",
    "                name: (lower_error_values, upper_error_values)\n",
    "            }\n",
    "            where `name` is a string representing the name of the experiment and `lower_error_values` and `upper_error_values` are lists or numpy arrays of error bar values.\n",
    "        title (str): The title of the plot.\n",
    "        key (Optional[Callable[[str], Tuple[int, float]]], optional): A function used to sort the names of the experiments.\n",
    "            The function should take a string representing the name of the experiment and return a tuple of integers `(n_layer, lr)`.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: The plot as a Plotly figure object.\n",
    "\n",
    "    Notes:\n",
    "        - The function sets the x-axis title to \"Number of Interaction Steps\" and the y-axis title to \"Accumulated Reward\".\n",
    "        - The function sorts the names of the experiments based on the `key` function if provided, otherwise the names are sorted alphabetically.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for name in sorted(y, key=key):\n",
    "        if \"Tanh\" in name:\n",
    "            n_layer, lr, _ = name.split(\" | \")\n",
    "        else:\n",
    "            n_layer, lr = name.split(\" | \")\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=f\"n layer: {n_layer}\",\n",
    "                x=x,\n",
    "                y=y[name],\n",
    "                mode=\"lines\",\n",
    "                line=dict(\n",
    "                    color=f\"rgb{str(COLORS[int(n_layer)])}\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=f\"upper {name}\",\n",
    "                x=x,\n",
    "                y=y_error[name][1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(\n",
    "                    width=0\n",
    "                ),\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=f\"lower {name}\",\n",
    "                x=x,\n",
    "                y=y_error[name][0],\n",
    "                mode=\"lines\",\n",
    "                fillcolor=f\"rgba{str(COLORS[int(n_layer)])[:-1]},0.1)\",\n",
    "                fill=\"tonexty\",\n",
    "                line=dict(\n",
    "                    width=0\n",
    "                ),\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.989,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Number of Interaction Steps\",\n",
    "            tickangle=-35,\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Accumulated Reward\",\n",
    "            tickangle=-35,\n",
    "            ticksuffix=\"  \",\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            yanchor=\"bottom\",\n",
    "            y=0.05,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            font_size=FONT_SIZE_LEGEND\n",
    "        ),\n",
    "        height=700,\n",
    "        width=900,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=20,\n",
    "            b=30,\n",
    "            t=40,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630b428-8212-4450-a933-0b77045499cf",
   "metadata": {},
   "source": [
    "### Memory Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84f454-dc71-4374-ac8a-01d156f92f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_idx = run_df[run_df[\"name\"] == \"spring-sweep-8\"][\"global_step\"].values\n",
    "\n",
    "mem_iqm_scores, mem_iqm_cis = rly.get_interval_estimates(\n",
    "    run_dict[\"Memory\"],\n",
    "    iqm,\n",
    "    reps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a7ea4-d9c0-4ddc-812c-bff3347a681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_combined_mem = plot_iqm(\n",
    "    x=mem_idx,\n",
    "    y=mem_iqm_scores,\n",
    "    y_error=mem_iqm_cis,\n",
    "    title=\"MiniGrid - Memory\",\n",
    "    key=lambda x: (int(x[:2]), float(x[-6:]))\n",
    ")\n",
    "\n",
    "world_combined_mem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525c9a4-01c8-41ad-b055-fcd312c3c199",
   "metadata": {},
   "source": [
    "### Continuous Recognition Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ae506-dacd-435d-a50f-efa1cdbaff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_idx = run_df[run_df[\"name\"] == \"dandy-sweep-5\"][\"global_step\"].values\n",
    "\n",
    "cr_iqm_scores, cr_iqm_cis = rly.get_interval_estimates(\n",
    "    run_dict[\"Continuous Recognition\"],\n",
    "    iqm,\n",
    "    reps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77ff2b-a38b-4fac-831a-ddfc0e351882",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_combined_psy = plot_iqm(\n",
    "    x=cr_idx,\n",
    "    y=cr_iqm_scores,\n",
    "    y_error=cr_iqm_cis,\n",
    "    title=\"PsychLab - Continuous Recognition\",\n",
    "    key=lambda x: (int(x[:2]), float(x[-6:]))\n",
    ")\n",
    "\n",
    "world_combined_psy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8b089-392d-42df-adb4-cf76b0bed495",
   "metadata": {},
   "source": [
    "## Preformance vs. Runtime\n",
    "Because some models perform nearly identical it is important to also highlight the differences in runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45464b17-6351-4394-8954-dda2dfb0b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_v_time(\n",
    "        data: Dict[str, Dict[str, Dict[str, float]]],\n",
    "        title: str\n",
    "    ) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plots performance vs time using the given data.\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing the data to be plotted.\n",
    "            The dictionary should have the following structure:\n",
    "            {\n",
    "                env_name: {\n",
    "                    name: {\n",
    "                        \"iqm\": float,\n",
    "                        \"runtime\": float,\n",
    "                        \"ci_high\": float,\n",
    "                        \"ci_low\": float,\n",
    "                        \"std\": float\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        title (str): The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: The plot as a Plotly figure object.\n",
    "\n",
    "    Notes:\n",
    "        - The function sorts the names of the experiments based on the first two characters and the last six characters.\n",
    "        - The function sets the y-axis range to [0, 1] if the title contains \"Memory\", and to [0, 47] otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    env_name = title.split(\" - \")[1]\n",
    "    env = tech_names[env_name]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    sorted_names = sorted(data[env], key=lambda x: (int(x[:2]), float(x[-6:])))\n",
    "    \n",
    "    y = [data[env][name][\"iqm\"] for name in sorted_names]\n",
    "    x = [data[env][name][\"runtime\"] / 3600 for name in sorted_names]\n",
    "    array = [abs(data[env][name][\"iqm\"] - data[env][name][\"ci_high\"]) for name in sorted_names]\n",
    "    arrayminus = [abs(data[env][name][\"iqm\"] - data[env][name][\"ci_low\"]) for name in sorted_names]\n",
    "    array_x = [data[env][name][\"std\"] / 3600 for name in sorted_names]\n",
    "    \n",
    "    symbols = [\"circle\", \"square\", \"diamond\", \"cross\", \"pentagon\"]\n",
    "    symbols.extend([f\"{sym}-open\" for sym in symbols])\n",
    "\n",
    "    for name in sorted_names:\n",
    "        n_layer = int(name.split(' | ')[0])\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=f\"n_layer: {n_layer}\",\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=11,\n",
    "                    symbol=\"diamond\",\n",
    "                    line_width=2,\n",
    "                    line_color=f\"rgb{COLORS_DARKER[n_layer]}\",\n",
    "                    color=f\"rgb{COLORS[n_layer]}\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"markers\",\n",
    "            error_x=dict(\n",
    "                array=array_x,\n",
    "                symmetric=True,\n",
    "                color=\"black\",\n",
    "                thickness=0.7\n",
    "            ),\n",
    "            error_y=dict(\n",
    "                arrayminus=arrayminus,\n",
    "                array=array,\n",
    "                symmetric=False,\n",
    "                color=\"black\",\n",
    "                thickness=0.7\n",
    "            ),\n",
    "            marker=dict(\n",
    "                size=13,\n",
    "                color=[f'rgb{COLORS[int(l.split(\" | \")[0])]}' for l in sorted_names],\n",
    "                symbol=\"diamond\",\n",
    "                line_width=2,\n",
    "                line_color=[f'rgb{COLORS_DARKER[int(l.split(\" | \")[0])]}' for l in sorted_names],\n",
    "            ),\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.989,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Runtime [h]\",\n",
    "            tickangle=-35,\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Accumulated Reward\",\n",
    "            tickangle=-35,\n",
    "            ticksuffix=\"  \",\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "            range=[0., 1.] if \"Memory\" in title else [0., 47.]\n",
    "        ),\n",
    "        legend=dict(\n",
    "            yanchor=\"bottom\",\n",
    "            y=0.01,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99,\n",
    "            bgcolor=\"rgba(0,0,0,0)\",\n",
    "            traceorder=\"reversed\",\n",
    "            font_size=FONT_SIZE_LEGEND\n",
    "        ),\n",
    "        height=700,\n",
    "        width=900,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=20,\n",
    "            b=30,\n",
    "            t=40,\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbf8db-2d2f-4547-aa2f-ba692b0ca38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_times_box(\n",
    "        data: Dict[str, List[float]],\n",
    "        title: str,\n",
    "        xaxis_title: str,\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Plots the rollout or update times as a box plot.\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, List[float]]): A dictionary containing the rollout times for each layer.\n",
    "            The dictionary should have the following structure:\n",
    "            {\n",
    "                layer: times\n",
    "            }\n",
    "            where `layer` is a string representing the layer and `times` is a list of rollout times.\n",
    "        title (str): The title of the plot.\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: The plot as a Plotly figure object.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for layer, times in data.items():\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=f\"n layer: {layer}\",\n",
    "                x=times,\n",
    "                marker_color=f\"rgb{COLORS[int(layer)]}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.98,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=xaxis_title,\n",
    "            tickangle=-35,\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            ticksuffix=\"  \",\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=700,\n",
    "        width=900,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=20,\n",
    "            b=30,\n",
    "            t=50,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf292297-0f7d-4a33-90d7-5ce0bfcdb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_and_error(\n",
    "        data: Dict[str, Dict[str, float]],\n",
    "        title: str,\n",
    "        yaxis_title: str\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Generates a bar plot with error bars based on the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (Dict[str, Dict[str, float]]): A dictionary containing the data to be plotted. The keys represent the layers, and the values are dictionaries with the following keys:\n",
    "        - iqm (float): The value for the y-axis.\n",
    "        - ci_low (float): The lower bound of the confidence interval.\n",
    "        - ci_high (float): The upper bound of the confidence interval.\n",
    "        - runtime (float): The runtime value.\n",
    "        - std (float): The standard deviation value.\n",
    "    - title (str): The title of the plot.\n",
    "    - yaxis_title (str): The title of the y-axis.\n",
    "\n",
    "    Returns:\n",
    "    - fig (go.Figure): The generated plot as a Plotly figure object.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    y_error_neg = []\n",
    "    y_error_pos = []\n",
    "    colors = []\n",
    "    for layer in sorted(data, key=lambda x: int(x.split(\" \")[0])):\n",
    "        plotting_data = data[layer]\n",
    "        layer = int(layer.split(' | ')[0])\n",
    "        x_value = f\"Layer {layer}\"\n",
    "        y_value = plotting_data[\"iqm\"] if \"Reward\" in yaxis_title else plotting_data[\"runtime\"] / 3600\n",
    "        y_error_neg_value = plotting_data[\"iqm\"] - plotting_data[\"ci_low\"] if \"Reward\" in yaxis_title else plotting_data[\"std\"] / 3600\n",
    "        y_error_pos_value = plotting_data[\"ci_high\"] - plotting_data[\"iqm\"] if \"Reward\" in yaxis_title else plotting_data[\"std\"] / 3600\n",
    "        color = f\"rgb{COLORS[layer]}\"\n",
    "        \n",
    "        x.append(x_value)\n",
    "        y.append(y_value)\n",
    "        y_error_pos.append(y_error_pos_value)\n",
    "        y_error_neg.append(y_error_neg_value)\n",
    "        colors.append(color)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            error_y=dict(\n",
    "                array=y_error_pos,\n",
    "                arrayminus=y_error_neg if \"Reward\" in yaxis_title else None,\n",
    "                symmetric=\"Reward\" not in yaxis_title\n",
    "            ),\n",
    "            marker_color=colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font_size=FONT_SIZE_TITLE,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5,\n",
    "            yanchor=\"top\",\n",
    "            y=0.98,\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            # tickangle=-35,\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=yaxis_title,\n",
    "            ticksuffix=\"  \",\n",
    "            tickfont_size=FONT_SIZE_TICKS,\n",
    "            title_font_size=FONT_SIZE_AXIS,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        height=700,\n",
    "        width=900,\n",
    "        margin=dict(\n",
    "            l=10,\n",
    "            r=20,\n",
    "            b=30,\n",
    "            t=50,#20\n",
    "            pad=10\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35475a2-c43a-4c45-849a-bf8c27e15f01",
   "metadata": {},
   "source": [
    "### Memory Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5c79c-694b-48aa-a735-e44794232c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_v_runtime_memory = plot_perf_v_time(perf_vs_time, \"MiniGrid - Memory\")\n",
    "\n",
    "perf_v_runtime_memory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f6901-2823-4670-8240-8335ef6ec49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_bar_mem = plot_bar_and_error(\n",
    "    perf_vs_time[\"MiniGrid-MemoryS11-v0\"],\n",
    "    \"MiniGrid - Memory\",\n",
    "    \"Accumulated Reward\"\n",
    ")\n",
    "\n",
    "reward_bar_mem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c4914-998a-45fb-9cd4-f35e8b98475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_bar_mem = plot_bar_and_error(\n",
    "    perf_vs_time[\"MiniGrid-MemoryS11-v0\"],\n",
    "    \"MiniGrid - Memory\",\n",
    "    \"Runtime [h]\"\n",
    ")\n",
    "\n",
    "\n",
    "runtime_bar_mem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d96561-f489-459b-855d-cf9c44fce180",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_times_mem = plot_times_box(rollout_times[tech_names[\"Memory\"]], \"MiniGrid - Memory\", \"Rollout time [sec]\")\n",
    "\n",
    "rollout_times_mem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3762f2f-b5b5-4ab5-bdf0-e40c22219405",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_times_mem = plot_times_box(update_times[tech_names[\"Memory\"]], \"MiniGrid - Memory\", \"Update time [sec]\")\n",
    "\n",
    "update_times_mem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c1fc1-31ee-49af-8be7-4cdc56983133",
   "metadata": {},
   "source": [
    "### Continuous Recognition Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e82c4-7a46-4335-8442-848a64ac8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_v_runtime_psych = plot_perf_v_time(perf_vs_time, \"PsychLab - Continuous Recognition\")\n",
    "\n",
    "perf_v_runtime_psych.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a3915-77f6-4070-bf94-bf7df64d63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_bar_psy = plot_bar_and_error(\n",
    "    perf_vs_time[\"psychlab_continuous_recognition\"],\n",
    "    \"PsychLab - Continuous Recognition\",\n",
    "    \"Accumulated Reward\"\n",
    ")\n",
    "\n",
    "reward_bar_psy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10708ea3-7b1d-44f2-a4d1-018b53d7768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_bar_psy = plot_bar_and_error(\n",
    "    perf_vs_time[\"psychlab_continuous_recognition\"],\n",
    "    \"PsychLab - Continuous Recognition\",\n",
    "    \"Runtime [h]\"\n",
    ")\n",
    "\n",
    "runtime_bar_psy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735621f-4189-4c3f-96c7-b2bf0b330da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_times_psy = plot_times_box(rollout_times[tech_names[\"Continuous Recognition\"]], \"PsychLab - Continuous Recognition\", \"Rollout time [sec]\")\n",
    "\n",
    "rollout_times_psy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5737321-bdc1-4efd-99d5-50f9f3122855",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_times_psy = plot_times_box(update_times[tech_names[\"Continuous Recognition\"]], \"PsychLab - Continuous Recognition\", \"Update time [sec]\")\n",
    "\n",
    "update_times_psy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49a0df-8fcf-489f-94ad-f2195f78d979",
   "metadata": {},
   "source": [
    "## Significance Test\n",
    "A statistical test is used to find any significant differences in the model variant performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57036ee3-f23f-4993-8afd-0fb8bbfe0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "CORRECT = True\n",
    "\n",
    "def highlight_sign(cell):\n",
    "    if type(cell) != str and cell < ALPHA :\n",
    "        return 'background: lightgreen; color: black'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e605f28-9b8d-4c42-8c19-3e3a25b16ecc",
   "metadata": {},
   "source": [
    "### Memory Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf6611-b3e2-4b6f-8332-438ee43e4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = {}\n",
    "\n",
    "for n1 in sorted(run_dict[\"Memory\"], key=lambda x: int(x.split(\" \")[0])):\n",
    "    sign[n1] = []\n",
    "    for n2, data in sorted(run_dict[\"Memory\"].items(), key=lambda x: int(x[0].split(\" \")[0])):\n",
    "        result = ttest_ind(\n",
    "            a=run_dict[\"Memory\"][n1][:,-1],\n",
    "            b=run_dict[\"Memory\"][n2][:,-1],\n",
    "            equal_var=False,\n",
    "            alternative=\"greater\"\n",
    "        )\n",
    "        sign[n1].append(result.pvalue if n1 != n2 else np.nan)\n",
    "\n",
    "if not CORRECT:\n",
    "    display(pd.DataFrame(data=sign, index=[model for model in sign]).T.style.applymap(highlight_sign))\n",
    "else:\n",
    "    p_vals = pd.DataFrame(data=sign, index=[model for model in sign]).values.flatten()\n",
    "    _, pps, *_ = multipletests(p_vals, alpha=0.05, method='holm')\n",
    "    result_df = pd.DataFrame(\n",
    "        pps.reshape(7,7),\n",
    "        index=[model for model in sign],\n",
    "        columns=[model for model in sign]\n",
    "    )\n",
    "    display(result_df.T.style.applymap(highlight_sign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa879b-76a2-4ad5-85f7-bd3a6804499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = {}\n",
    "\n",
    "for n1 in sorted(run_dict[\"Memory\"], key=lambda x: int(x.split(\" \")[0])):\n",
    "    sign[n1] = []\n",
    "    for n2, data in sorted(run_dict[\"Memory\"].items(), key=lambda x: int(x[0].split(\" \")[0])):\n",
    "        result = mannwhitneyu(\n",
    "            run_dict[\"Memory\"][n1][:,-1],\n",
    "            run_dict[\"Memory\"][n2][:,-1],\n",
    "            alternative=\"greater\"\n",
    "        )\n",
    "        sign[n1].append(result.pvalue if n1 != n2 else np.nan)\n",
    "\n",
    "if not CORRECT:\n",
    "    display(pd.DataFrame(data=sign, index=[model for model in sign]).T.style.applymap(highlight_sign))\n",
    "else:\n",
    "    p_vals = pd.DataFrame(data=sign, index=[model for model in sign]).values.flatten()\n",
    "    _, pps, *_ = multipletests(p_vals, alpha=0.05, method='holm')\n",
    "    result_df = pd.DataFrame(\n",
    "        pps.reshape(7,7),\n",
    "        index=[model for model in sign],\n",
    "        columns=[model for model in sign]\n",
    "    )\n",
    "    display(result_df.T.style.applymap(highlight_sign))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8232f8c-e5cb-42d0-a8cd-bb2a1a207155",
   "metadata": {},
   "source": [
    "### Continuous Recognition Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae883b-517c-4017-a2d2-9b7549880b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = {}\n",
    "\n",
    "for n1 in sorted(run_dict[\"Continuous Recognition\"], key=lambda x: int(x.split(\" \")[0])):\n",
    "    sign[n1] = []\n",
    "    for n2, data in sorted(run_dict[\"Continuous Recognition\"].items(), key=lambda x: int(x[0].split(\" \")[0])):\n",
    "        result = ttest_ind(\n",
    "            a=run_dict[\"Continuous Recognition\"][n1][:,-1],\n",
    "            b=run_dict[\"Continuous Recognition\"][n2][:,-1],\n",
    "            equal_var=False,\n",
    "            alternative=\"greater\"\n",
    "        )\n",
    "        sign[n1].append(result.pvalue if n1 != n2 else np.nan)\n",
    "\n",
    "if not CORRECT:\n",
    "    display(pd.DataFrame(data=sign, index=[model for model in sign]).T.style.applymap(highlight_sign))\n",
    "else:\n",
    "    p_vals = pd.DataFrame(data=sign, index=[model for model in sign]).values.flatten()\n",
    "    _, pps, *_ = multipletests(p_vals, alpha=0.05, method='holm')\n",
    "    result_df = pd.DataFrame(\n",
    "        pps.reshape(3,3),\n",
    "        index=[model for model in sign],\n",
    "        columns=[model for model in sign]\n",
    "    )\n",
    "    display(result_df.T.style.applymap(highlight_sign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a6f35-88ad-484f-9b1e-b0bb757dbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = {}\n",
    "\n",
    "for n1 in sorted(run_dict[\"Continuous Recognition\"], key=lambda x: int(x.split(\" \")[0])):\n",
    "    sign[n1] = []\n",
    "    for n2, data in sorted(run_dict[\"Continuous Recognition\"].items(), key=lambda x: int(x[0].split(\" \")[0])):\n",
    "        result = mannwhitneyu(\n",
    "            run_dict[\"Continuous Recognition\"][n1][:,-1],\n",
    "            run_dict[\"Continuous Recognition\"][n2][:,-1],\n",
    "            alternative=\"greater\"\n",
    "        )\n",
    "        sign[n1].append(result.pvalue if n1 != n2 else np.nan)\n",
    "\n",
    "if not CORRECT:\n",
    "    display(pd.DataFrame(data=sign, index=[model for model in sign]).T.style.applymap(highlight_sign))\n",
    "else:\n",
    "    p_vals = pd.DataFrame(data=sign, index=[model for model in sign]).values.flatten()\n",
    "    _, pps, *_ = multipletests(p_vals, alpha=0.05, method='holm')\n",
    "    result_df = pd.DataFrame(\n",
    "        pps.reshape(3,3),\n",
    "        index=[model for model in sign],\n",
    "        columns=[model for model in sign]\n",
    "    )\n",
    "    display(result_df.T.style.applymap(highlight_sign))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
